# [[M2L2025](https://www.m2lschool.org/home)] Tutorial 2: Vision Language Models

**Authors:** Alexandre Galashov (agalashov@google.com), Petra Bevandic (Petra.Bevandic@fer.hr )

**Abstract**: In this tutorial we'll explore how we can use image-text data to build Vision Language Models (VLMs) ðŸš€. We'll start with an introduction to multimodal understanding that describes the main components of a Vision Lanugage Model. Then, we'll dive into Vision Transformer (ViT), a popular architecture which is used in VLMs as Image Encoder (Practical 1). It will be followed in Practical 2 which will dive into Contrastive Language-Image Pre-training (CLIP), a model for learning general representation from image-text pairs that can be used for a wide range of downstream tasks. We will also dive into different applications of CLIP. Finally, in Practical 3, we will actually use a pretrained VLM and do a finetuning to a given task.

--- 

### Outline


#### Theory recap

#### Part 1: A deep dive into Vision Transformer (ViT)

#### Part 2: CLIP -- basics and applications
- CLIP loss
- Image search with CLIP
- Zero-shot classification with CLIP
- Anomaly detection with CLIP
- Failure cases of CLIP

#### Part 3: Using VLMs -- Zero-shot & finetuning
- Zero-shot Amazon product descriptions generation with existing VLMs
- LoRA finetuning of VLMs


### Notebooks

#### Theory recap

Notebook: [![Open In 
Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/M2Lschool/tutorials2025/blob/master/2_vlm/vlm_tutorial_overview.ipynb)

#### Part 1:
Tutorial: [![Open In 
Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/M2Lschool/tutorials2025/blob/master/2_vlm/vlm_tutorial_practical_1_students.ipynb)

Solution: [![Open In 
Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/M2Lschool/tutorials2025/blob/master/2_vlm/vlm_tutorial_practical_1_solution.ipynb)


#### Part 2:
Tutorial: [![Open In 
Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/M2Lschool/tutorials2025/blob/master/2_vlm/vlm_tutorial_practical_2_students.ipynb)

Solution: [![Open In 
Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/M2Lschool/tutorials2025/blob/master/2_vlm/vlm_tutorial_practical_2_solution.ipynb)


#### Part 3:
Tutorial: [![Open In 
Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/M2Lschool/tutorials2025/blob/master/2_vlm/vlm_tutorial_practical_3_students.ipynb)

Solution: [![Open In 
Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/M2Lschool/tutorials2025/blob/master/2_vlm/vlm_tutorial_practical_3_solution.ipynb)

---

### Acknowledgement

This work has been supported by the German federal state of North Rhine-Westphalia as part of the research funding program KI-Starter.
