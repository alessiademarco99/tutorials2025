{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "M2L School 2025 Edition in Split, Croatia.\n",
        "\n",
        "This tutorial is based on:\n",
        "*   Explanations and code snippets from the [2024 edition](https://github.com/M2Lschool/tutorials2024/tree/main/4_rl)\n",
        "\n",
        "Notebook Author: Yuqing Du"
      ],
      "metadata": {
        "id": "aAu5tSdOUn3z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "# RL01: Introduction to Reinforcement Learning\n",
        "---"
      ],
      "metadata": {
        "id": "h-b5N_w_rsAC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this tutorial, we will be learning about Reinforcement Learning, a type of Machine Learning where an **agent** learns to choose **actions** in an **environment** that lead to maximal **reward** in the long run. RL has seen tremendous success on a wide range of challenging problems such as learning to play complex games like [Atari](https://www.deepmind.com/blog/agent57-outperforming-the-human-atari-benchmark), [StarCraft II](https://www.deepmind.com/blog/alphastar-mastering-the-real-time-strategy-game-starcraft-ii) and [Dota II](https://openai.com/five/) and [Go](https://www.nature.com/articles/nature16961).\n",
        "\n",
        "Unlike fields like supervised learning, where we give examples of expected behaviour to our models, RL focuses on *goal-orientated* learning from interactions, through trial-and-error. RL algorithms learn what to do (i.e., which actions to take) in an environment to maximise some reward signal. In settings like a video game, the reward signal could be the score of the game, i.e., RL algorithms will try to maximise the score in the game by choosing the best actions.  \n",
        "\n",
        "<center>\n",
        "<img src=\"https://miro.medium.com/max/1400/1*Ews7HaMiSn2l8r70eeIszQ.png\" width=\"40%\" />\n",
        "</center>\n",
        "\n",
        "[*Image Source*](https://towardsdatascience.com/multi-agent-deep-reinforcement-learning-in-15-lines-of-code-using-pettingzoo-e0b963c0820b)\n",
        "\n",
        "More precisely, in RL we have an **agent** which perceives an **observation** $o_t$ of the current state $s_t$ of the **environment** and must choose an **action** $a_t$ to take. The environment then transitions to a new state $s_{t+1}$ in response to the agent's action and also gives the agent a scalar reward $r_t$ to indicate how good or bad the chosen action was given the environment's state. The goal in RL is for the agent to maximise the amount of reward it receives from the environment over time. The subscript $t$ is used to indicate the timestep number. In this tutorial we will only consider fully observable games, which means thet state of the environment and observation of the agent are the same.\n",
        "\n",
        "We will cover the following sections in this tutorial:\n",
        "\n",
        "* **Environments** where we will either implement a simple environment or use a classical gym environment.\n",
        "* **Agent-Environment loop** where we will see how agent and environment interact. We will consider the simplest possible agent for this: the agent that just takes a random action at each step.\n",
        "* **Value-based reinforcement learning** where we will implement an agent that learns from its interactions with the environment which actions to take to achieve the highest reward."
      ],
      "metadata": {
        "id": "5b0_oW3ur3wz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this colab you will find some\n",
        "\n",
        "#### ⭐ Exercises\n",
        "where you need to implement missing parts in the code, or answer the quesions that test your understanding of code and algothims.\n",
        "When you need to complete some code, the section is marked as:\n",
        "\n",
        "```\n",
        "# -----------------------------------#\n",
        "# You code goes here\n",
        "# -----------------------------------#\n",
        "```\n",
        "with any comments to help you to complete the task. Sometimes it is useful to have a look at further code in the cell to understand which variables you need to assign in your implementation.\n",
        "Some exercises only require you to answer the questions or experiment with the code.\n",
        "You can skip bonus exercises if you are short on time as the rest of the content does not depend on them. You can use the Table of content on the left to vavigate the tutorial. Let's begin!\n"
      ],
      "metadata": {
        "id": "N2FW3_dKsBHD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Environments\n",
        "\n",
        "In reinforcement learning, we do not start with a dataset, but with an environment. Environments in RL represent the task or problem that we are trying to solve. There are many types of environments, such as board or computer games, simulated robotics settings, etc.\n",
        "\n",
        "In this tutorial, we will look in details at two simple environments: Minigolf and Cartpole."
      ],
      "metadata": {
        "id": "6uv4CWyvflTj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install gymnasium"
      ],
      "metadata": {
        "id": "btRzNlpGsuJ_"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import animation\n",
        "from matplotlib import rc\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import gymnasium as gym\n",
        "\n",
        "# making plots pretty\n",
        "sns.set_palette(\"deep\")\n",
        "rc('animation', html='jshtml')\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# for reproducibility's sake!\n",
        "random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "print(gym.__version__)"
      ],
      "metadata": {
        "id": "upoBaYysgvnH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8aba2be-3bd9-418c-ffc7-976858151d9f"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create your own environment\n",
        "\n",
        "We will firstly see an example of a simple, customized environment created for `gym`. We introduce some theory on the physics of putting (you don't have to focus on this) that will implemented by our environment.\n",
        "\n",
        "The `Minigolf` environment models a simple problem in which the agent has to hit a ball on grass using a putter in order to reach the hole with the minimum amount of moves.\n",
        "\n",
        "* The green (grass) is characterized by a **friction** $f$ that is selected uniformly random at the beginning of each episode in the interval `[0.065, 0.196]` and does not change during the episode.\n",
        "* The **position** of the ball is represented by a unidimensional variable $x_t$ that is initialized uniformly random in the interval `[1,20]`. The observation is made of the pair $s_t = (x_t,f)$.\n",
        "* The **action** $a_t$ is the force applied to the putter and has to be bounded in the interval `[1e-5,5]`. Before being applied the action is subject to a Gaussian noise, so that the actual action $u_t$ applied is given by:\n",
        "\n",
        "$$\n",
        "u_t = a_t + \\epsilon \\qquad \\text{where} \\qquad \\epsilon \\sim \\mathcal{N}(0,\\sigma^2),\n",
        "$$\n",
        "where $\\sigma =0.1$. The movement of the ball is governed by the kinematic law:\n",
        "\n",
        "$$\n",
        "x_{t+1} = x_{t} - v_t \\tau_t + \\frac{1}{2} d \\tau_t^2\n",
        "$$\n",
        "\n",
        "where:\n",
        "* $v_t$ is the velocity computed as $v_t = u_t l$,\n",
        "* $d$ is the deceleration computed as $d = \\frac{5}{7} fg$,\n",
        "* $\\tau_t$ is the time interval computed as $\\tau_t = \\frac{v_t}{d}$.\n",
        "\n",
        "The remaining constants are the putter length $l = 1$ and the gravitational acceleration $g=9.81$. The **episode** terminates when the next state is such that the ball enters or surpasses (without entering) the hole. The **reward** is `-1` at every step and `-100` if the ball surpasses the hole. To check whether the ball will not reach, enter, or surpass the hole, refer to the following condition:\n",
        "\n",
        "\\begin{align*}\n",
        "&v_t < v_{\\min} \\implies \\text{ball does not reach the hole} \\\\\n",
        "&v_t > v_{\\max} \\implies \\text{ball surpasses the hole} \\\\\n",
        "&\\text{otherwise} \\implies \\text{ball enters the hole}\n",
        "\\end{align*}\n",
        "\n",
        "where\n",
        "\n",
        "\\begin{align*}\n",
        "& v_\\min = \\sqrt{\\frac{10}{7} fgx_t}\n",
        "& v_\\max = \\sqrt{ \\frac{g(2 h - \\rho)^2}{2\\rho} + v_\\min^2},\n",
        "\\end{align*}\n",
        "where $h = 0.1$ is the hole size and $\\rho = 0.02135$ is the ball radius.\n",
        "\n",
        "\n",
        "**References**\n",
        "\n",
        "Penner, A. R. \"The physics of putting.\" Canadian Journal of Physics 80.2 (2002): 83-96."
      ],
      "metadata": {
        "id": "D3aGkDIvsPDn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ⭐ Exercise\n",
        "Skim through the custom Minigolf environment below. Try to fill the blank areas with information from the formulas above. While running the environment, you should notice the first observation value (x position) decreases over time."
      ],
      "metadata": {
        "id": "dmHCWCdKsUwQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from gymnasium.spaces import Box\n",
        "\n",
        "class Minigolf(gym.Env):\n",
        "    \"\"\"\n",
        "    The Minigolf problem.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Minigolf, self).__init__()\n",
        "\n",
        "        # Constants\n",
        "        self.min_pos, self.max_pos = 1.0, 20.0\n",
        "        self.min_action, self.max_action = 1e-5, 5.0\n",
        "        self.min_friction, self.max_friction = 0.065, 0.196\n",
        "        self.putter_length = 1.0\n",
        "        self.hole_size = 0.10\n",
        "        self.sigma_noise = 0.1\n",
        "        self.ball_radius = 0.02135\n",
        "\n",
        "\n",
        "        # Instance the spaces\n",
        "        low = np.array([self.min_pos, self.min_friction])\n",
        "        high = np.array([self.max_pos, self.max_friction])\n",
        "\n",
        "        self.action_space = Box(low=self.min_action,\n",
        "                                high=self.max_action,\n",
        "                                shape=(1,),\n",
        "                                dtype=np.float32)\n",
        "\n",
        "        self.observation_space = Box(low=low,\n",
        "                                     high=high,\n",
        "                                     shape=(2,),\n",
        "                                     dtype=np.float32)\n",
        "\n",
        "    def step(self, action):\n",
        "\n",
        "        # Retrieve the state components\n",
        "        x, friction = self.state\n",
        "\n",
        "        # Clip the action within the allowed range\n",
        "        action = torch.clip(\n",
        "            torch.tensor(action),\n",
        "            torch.tensor(self.min_action),\n",
        "            torch.tensor(self.max_action)\n",
        "        )\n",
        "\n",
        "        # Add noise to the action\n",
        "        noisy_action = action + torch.randn(1).item() * self.sigma_noise\n",
        "\n",
        "        # Compute the speed\n",
        "        v = noisy_action * self.putter_length\n",
        "        v = torch.ravel(torch.tensor(v)).item()\n",
        "\n",
        "        # -----------------------------------#\n",
        "        # Compute the speed limits v_min, v_max\n",
        "        # f = friction, g = 9.81, x_t = x\n",
        "        g = 9.81\n",
        "        v_min = torch.sqrt(10/7 * g*friction*x)\n",
        "        v_max = torch.sqrt((g*(2*self.hole_size - self.ball_radius)**2)/(2*self.ball_radius) + v_min**2)\n",
        "        # -----------------------------------#\n",
        "\n",
        "        # Compute the deceleration\n",
        "        deceleration = 5 / 7 * friction * 9.81\n",
        "\n",
        "        # Compute the time interval\n",
        "        t = v / deceleration\n",
        "\n",
        "        # Update the state and clip\n",
        "        x = x - v * t + 0.5 * deceleration * t ** 2\n",
        "        x = torch.clip(x, self.min_pos, self.max_pos)\n",
        "\n",
        "        # Compute the reward and episode termination\n",
        "        reward = 0.\n",
        "        done = True\n",
        "\n",
        "        if v < v_min:\n",
        "            reward = -1.\n",
        "            done = False\n",
        "        elif v > v_max:\n",
        "            reward = -100.\n",
        "\n",
        "        self.state = torch.tensor([x, friction]).float()\n",
        "\n",
        "        return self.state, reward, done, False, {}\n",
        "\n",
        "\n",
        "    def reset(self, seed=None):\n",
        "\n",
        "        # Random generation of initial position and friction\n",
        "        # scale [0, 1] to [min, max]\n",
        "        x = torch.rand(1) * (self.max_pos-self.min_pos) + self.min_pos\n",
        "        friction = torch.rand(1) * (self.max_friction-self.min_friction) + self.min_friction\n",
        "\n",
        "        self.state = torch.hstack([x, friction])\n",
        "\n",
        "        return self.state, {}"
      ],
      "metadata": {
        "id": "hn3eTtnSs004"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's instantiate the custom environment."
      ],
      "metadata": {
        "id": "VIezkwuWS4Lo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------#\n",
        "env = Minigolf()\n",
        "# -----------------------------------#\n",
        "\n",
        "obs = env.reset()\n",
        "observation, reward, terminated, truncated, info = env.step(torch.randn(1))"
      ],
      "metadata": {
        "id": "2K1Xlj2WtY4o",
        "outputId": "80b222b9-6607-4dd2-880c-5ddd16faf40f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'TODO' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3478626523.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# -----------------------------------#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTODO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# -----------------------------------#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'TODO' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What do these variables represent?\n",
        "\n",
        "\n",
        "*   `obs` is Tensor containing the representation of the current state of the environment. In `Minigolf`, this is equivalent to `Tensor([x_position, friction])`.\n",
        "*   `reward` is returned by the environment.\n",
        "*   `terminated` or `done` is a boolean variable determining whether the episode (interaction) is over, i.e. for this environment that the ball enters or surpasses (without entering) the hole.\n",
        "*   `truncated` is a boolean variable determining whether the episode (interaction) is over outside the scope of the environment itself, e.g. reaching a time limit. We do not implement this for the Minigolf environment.\n",
        "* `info` is a dictionary that may contain additional information about the step, such as any relevant metrics or debugging info. May be empty.\n",
        "\n"
      ],
      "metadata": {
        "id": "Nonb5ijdTRQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ⭐ Exercise\n",
        "Let's now write a simple interaction loop:\n",
        "\n",
        "\n",
        "1.   Reset the environment\n",
        "2.   While the episode is not over, perform an environment step with a fixed or random action\n",
        "\n"
      ],
      "metadata": {
        "id": "_pZGceipTF5Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "obs = env.reset()\n",
        "is_done = False\n",
        "step = 0\n",
        "total_reward = 0\n",
        "while not is_done:\n",
        "  # -----------------------------------#\n",
        "  # Take a step in the environment with a random or fixed action.\n",
        "  obs, reward, is_done, _, info = TODO\n",
        "  # -----------------------------------#\n",
        "  print(f\"step {step}, obs: {obs}\")\n",
        "  step += 1\n",
        "  total_reward += reward\n",
        "print(f\"Total reward: {total_reward}\")"
      ],
      "metadata": {
        "id": "QZ80CDSFTBtp",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have just introduced a simple, textual environment that can be imported in `gym` to train RL algorithms. We will now overview existing environments with more complex states (numeric, visual)."
      ],
      "metadata": {
        "id": "Kif8er3Nl7WT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use one of the gym environments\n",
        "\n",
        "We will use a famous [CartPole](https://gymnasium.farama.org/environments/classic_control/cart_pole/) or inverted pendulum environment.\n",
        "\n",
        "<center><img src=\"https://user-images.githubusercontent.com/10624937/42135683-dde5c6f0-7d13-11e8-90b1-8770df3e40cf.gif\" height=\"250\" /></center>\n",
        "\n",
        "From the documentation:\n",
        "\n",
        "```A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The pendulum is placed upright on the cart and the goal is to balance the pole by applying forces in the left and right direction on the cart. The action ... can take values $\\{0, 1\\}$ indicating the direction of the fixed force the cart is pushed with. The observation ... includes the values corresponding to the following positions and velocities: Cart Position, Cart Velocity, Pole Angle, Pole Angular Velocity. Since the goal is to keep the pole upright for as long as possible, a reward of $+1$ for every step taken, including the termination step, is allotted. At the starting state all observations are assigned a uniformly random value in $(-0.05, 0.05)$.```\n",
        "\n",
        "Please heck the [documentation](https://gymnasium.farama.org/environments/classic_control/cart_pole/) to see more details about the environment."
      ],
      "metadata": {
        "id": "NVyMphj6N9Rz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# instantiating the environment object\n",
        "env = gym.make('CartPole-v1', render_mode=\"rgb_array\")"
      ],
      "metadata": {
        "id": "5p3Re9298HfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this resets all env variables and samples the initial state\n",
        "episode = 0\n",
        "step = 0\n",
        "obs = env.reset()\n",
        "\n",
        "# a in {0, 1}, check: https://www.gymlibrary.dev/environments/classic_control/cart_pole/\n",
        "action = 0\n",
        "\n",
        "# get update tuple from the env: (s':List[float], r(s,a):float, terminated:bool, truncated:bool, info:dict)\n",
        "obs, reward, terminated, truncated, info = env.step(action) # this is just ONE step!\n",
        "step += 1\n",
        "\n",
        "# one episode goes from the initial state to the ending state\n",
        "# the environment determines whether you are done with the flags: terminated, truncated\n",
        "# this follows the game logic, e.g. in CartPole, the stick fell out of the cart.\n",
        "is_episode_done = truncated or terminated\n",
        "\n",
        "print(f\"\"\"\n",
        "+===================+\n",
        "Episode:\\t{episode}\n",
        "Step:\\t\\t{step}\n",
        "Is done:\\t{is_episode_done}\n",
        "Reward:\\t\\t{reward}\n",
        "Info:\\t\\t{info}\n",
        "+===================+\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "fMAsrZMB9KFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.axis(\"off\")\n",
        "plt.imshow(env.render())"
      ],
      "metadata": {
        "id": "TUt67Vw5A5Hl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Agent-Environment loop\n",
        "We now turn to the agent. An agent receives the current **state** from the environment, and uses an internal **policy** to determine an **action** to take. We implement the agent as a Python [class](https://en.wikibooks.org/wiki/A_Beginner%27s_Python_Tutorial/Classes), which is just a logical wrapper of variables and methods (functions) that operate on those variables. The methods our first agent will have are the following:\n",
        "\n",
        "\n",
        "* ```__init__```:  Initialises the agent the first time it's created.\n",
        "* `action_step`: Receives the observation from the environment and returns an action.\n",
        "\n"
      ],
      "metadata": {
        "id": "dL8vaby9QVmb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Agent\n",
        "\n",
        "To get a feel for an agent and the methods it has, let's first implement an agent that ignores the observations and just takes a *random* action at every step: for example, for CartPole it decides to go right or left with equal probability."
      ],
      "metadata": {
        "id": "Mzc-4VrCUQgq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ⭐ Exercise\n",
        "Now it is your turn to create:\n",
        "\n",
        "*   An **agent function** or class that given the observation, it **picks and returns an action** among the acceptable ones. Right now you don't have to implement any logic: generate **random** values with PyTorch and remember that gym accepts default python types (float, list).\n",
        "*   A double agent-env **interaction loop**: iterate through a defined number of episodes, thus create an inner loop to run through the **episode steps** as the game is not done yet.\n",
        "\n",
        "Your interaction loop should be part of the function `evaluate_policy` and it should return a list of episode rewards (you can sum all the step rewards within an episode).\n",
        "\n",
        "To show interactions between the agent and the environment in the form of animations, you can create an array of rgb frames with the line: `frames.append(env.render(mode=\"rgb_array\"))`.\n",
        "\n"
      ],
      "metadata": {
        "id": "KQEjkXQY-JST"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from abc import ABC, abstractmethod\n",
        "\n",
        "class Policy(ABC):\n",
        "    @abstractmethod\n",
        "    def action_step(self):\n",
        "        pass\n",
        "    @abstractmethod\n",
        "    def learner_step(self):\n",
        "        pass"
      ],
      "metadata": {
        "id": "VnDgukilu3f8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_actions = env.action_space.n\n",
        "n_actions"
      ],
      "metadata": {
        "id": "s9jjmM_DvtkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RandomPolicy(Policy):\n",
        "  def action_step(self, obs):\n",
        "    # -----------------------------------#\n",
        "    # sample a random number in the range of n_actions\n",
        "    return np.random.randint(n_actions)\n",
        "    # -----------------------------------#\n",
        "  def learner_step(self,):\n",
        "    pass # no learning involved at the moment"
      ],
      "metadata": {
        "id": "vtOLd2Gg977t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Utility functions to evaluate policies\n",
        "def evaluate_policy(policy, num_episodes):\n",
        "  all_episode_rewards = []\n",
        "  progressbar = tqdm(range(num_episodes))\n",
        "  for i in progressbar: # iterate over the episodes\n",
        "    episode_rewards = []\n",
        "    done = False\n",
        "    obs = env.reset()\n",
        "    frames = []\n",
        "    while not done: # iterate over the steps until termination\n",
        "        action = policy.action_step(obs)\n",
        "        obs, reward, terminated, _, _ = env.step(action)\n",
        "        done = terminated\n",
        "        episode_rewards.append(reward) # compute discounted reward\n",
        "        try:\n",
        "          frame = env.render()\n",
        "          frames.append(frame)\n",
        "        except:pass\n",
        "    all_episode_rewards.append(sum(episode_rewards))\n",
        "  return all_episode_rewards, frames\n",
        "\n",
        "def plot_rewards(rewards, avg_window_size=10):\n",
        "  plt.clf()\n",
        "  moving_avg_rewards = torch.tensor(rewards).unfold(dimension=0, size=avg_window_size, step=1).mean(dim=0)\n",
        "  plt.plot(range(moving_avg_rewards.shape[0]), moving_avg_rewards.tolist())\n",
        "  plt.xlabel(\"Episodes\")\n",
        "  plt.ylabel(\"Reward\")\n",
        "  plt.grid()\n",
        "  plt.show()\n",
        "\n",
        "def animate(data, interval=200):\n",
        "  fig = plt.figure(1)\n",
        "  img = plt.imshow(data[0])\n",
        "  plt.axis('off')\n",
        "\n",
        "  def animate(i):\n",
        "    img.set_data(data[i])\n",
        "\n",
        "  anim = animation.FuncAnimation(fig, animate, frames=len(data), interval=interval)\n",
        "  plt.close(1)\n",
        "  return anim"
      ],
      "metadata": {
        "id": "S8_vyakIg8nm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_episodes = 100 # @param\n",
        "random_policy_rewards, frames = evaluate_policy(\n",
        "    policy=RandomPolicy(), num_episodes=num_episodes)"
      ],
      "metadata": {
        "id": "tCKAkmwEluKU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_random_policy_rwd = torch.tensor(random_policy_rewards).mean()\n",
        "print(f\"Average evaluation reward: {eval_random_policy_rwd}\")"
      ],
      "metadata": {
        "id": "kEsQPeAFj2-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_rewards(random_policy_rewards, 100)"
      ],
      "metadata": {
        "id": "3_VmMDbPMRtm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can finally render one episode of agent-environment interaction. Does the agent manage to keep the pole in balance?"
      ],
      "metadata": {
        "id": "7mTEnW8jUqEh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "animate(frames)"
      ],
      "metadata": {
        "id": "4Xx9L8UEO9wo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Value-Based Reinforcement Learning\n",
        "\n",
        "Not surprisingly, our random agent is not really good at this game and we need to use some learning.\n",
        "\n",
        "In **value-based** reinforcement learning methods, agents maintain a **value** for all **state-action** pairs. A **value** of a **state-action** pair is telling us what reward to expect if we start at this **state**, take this action and afterwards follow a policy. Then, we use these **value** estimates to choose actions that maximise the value.\n",
        "\n",
        "\n",
        "## Q-learning\n",
        "\n",
        "One efficient algorithm for value-based learning is [Q-learning](https://en.wikipedia.org/wiki/Q-learning).\n",
        "The function that mapping state-action pairs to values for a specific policy $\\pi$ is called **Q-function**.\n",
        "Formally, **Q-function** is defined as:\n",
        "\n",
        "$$ Q^{\\pi}(s,a) = \\mathbb{E}_{\\tau \\sim P^{\\pi}} \\left[ \\sum_t \\gamma^t R_t| s_0=s,a=a_0 \\right]$$\n",
        "\n",
        "where $\\tau = \\{s_0, a_0, r_0, s_1, a_1, r_1, \\cdots \\}$. In other words, $Q^{\\pi}(s,a)$ is the expected **value** (sum of discounted rewards) of being in a given **state** $s$ and taking the **action** $a$ and then following policy ${\\pi}$ thereafter.\n",
        "\n",
        "Given a Q-function, it is easy to construct a good policy. For example, a greedy policy selects the action that maximises the Q-function estimate:\n",
        "$$\\pi_{greedy} (a|s) = \\arg\\max_a Q^{\\pi}(s,a). $$\n",
        "\n",
        "The value $V^\\pi$ of a state is the expected $Q^\\pi$ over possible actions:\n",
        "\n",
        "$$V^\\pi(s) = \\sum_{a \\in A} \\pi(a |s) Q^\\pi(s,a)$$\n",
        "\n",
        "We will reply on famous Bellman Optimality Equation to estimate the Q-function:\n",
        "\n",
        "$$ Q^\\pi(s,a) =  r(s,a) + \\gamma  \\sum_{s'\\in S} P(s' |s,a) V^\\pi(s'). $$\n",
        "\n",
        "It breaks down $Q^{\\pi}(s,a)$ into 2 parts: the immediate reward associated with being in state $s$ and taking action $a$, and the discounted sum of all future rewards. Then, to learn the Q-function we can use [temporal difference (TD) learning](https://en.wikipedia.org/wiki/Temporal_difference_learning).\n",
        "If we are given a sample of state $s$, action $a$, reward $r(s,a)$ and next state $s'$, we compute the TD-error $\\delta$ of a policy $\\pi_e$ as:\n",
        "\n",
        "$$\\delta = r(s,a) + \\gamma Q(s', \\underbrace{\\pi_e(s'}_{a'})) − Q(s, a).$$\n",
        "The first two terms is the Q-function estimate of $Q(s,a)$ through the immediate reward and Q-function of the next state, and the third term is the direct estimate of the Q-function.\n",
        "\n",
        "Then, we will update the $Q$ value estimates at each step with the following update rule:\n",
        "\n",
        "$$Q(s, a) \\gets Q(s, a) + \\alpha \\delta, $$\n",
        "\n",
        "where $\\delta$ is a TD-error and $\\alpha$ is a small learning step size will influence how quickly our $Q$ values will be updated given new observations.\n"
      ],
      "metadata": {
        "id": "aHdaGS9bEhEI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tabular Q-learning\n",
        "\n",
        "We will start with tabular Q-learning where Q-function is represented by a table with a value for every state and action. For this, we will need to enumerate all possible states and actions."
      ],
      "metadata": {
        "id": "H54uGf0xVZ8t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Steps to implement Q-learning agent\n",
        "\n",
        "We will modify the Random Agent in the following way:\n",
        "\n",
        "1. **Represent Q values.** Our state space consists in the position and velocity of the cart and the pole, $(x_c, v_c, x_p, v_p)$, which are going to be the dimensions of our complex q-table (or tensor). Note that the values in our observation tuple are continuous, while the size of a tensor are finite. *How to map the return of an action to all possible values of e.g. $x_c$?* We need to **discretize** continuous values by mapping them to intervals or bins $N_b$. Our final Q-table will consist in a 5-dimensional tensor of size $(N_{b^0}, N_{b^1}, N_{b^2}, N_{b^3}, |A|)$, the 5-th dimension is the number of actions $a \\in A$.\n",
        "\n",
        "2. **Implement a policy.** We will use a greedy policy that returns the action with the highest $Q$ value.\n",
        "\n",
        "3. **Implement a learning step.** We need to add a new method to our agent class to do the learning step which updates the $Q$ values based on the data. We will call this new method  `learner_step`."
      ],
      "metadata": {
        "id": "e7qSVQYEVdVQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ⭐ Exercise"
      ],
      "metadata": {
        "id": "jCxYjbBkDCH1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have pre-defined a QLearningPolicy template class that maps continuous states into the Q-Table for you (`_get_q_idx`, `_get_q`, `_set_q`). Get familiar with the code and complete the following functions:\n",
        "\n",
        "\n",
        "*   `actor_step`: given the observation tensor, take the index of the action with the highest utility.\n",
        "*   `learner_step`: given $(s, a, r(s,a), s')$, update the Q-table.\n",
        "\n",
        "Compute first the temporal difference error and use it to compute the Q-function update as defined in the equation above.\n",
        "\n",
        "*How to handle discretization in practice?*\n",
        "\n",
        "We have written the function `_get_q_idx` for you. It discretizes a state representation by normalizing the values with constants. A general approach to discretization is binning, you can read more about it with PyTorch [here](https://pytorch.org/docs/stable/generated/torch.bucketize.html).\n",
        "\n",
        "The difference with Deep Q-Learning lies also here: learning the value function by mapping complex state representations, such as RGB images, to values. To handle high-dimensional inputs, different neural network architectures (e.g. Convolutional) are used."
      ],
      "metadata": {
        "id": "K6CqPkewXQYH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class QLearningPolicy:\n",
        "    def __init__(\n",
        "        self,\n",
        "        discount: float,\n",
        "        lr: float,\n",
        "        n_actions: int,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Implementation of the Q-Learning Policy\n",
        "        Args:\n",
        "            discount: the discouting factor\n",
        "            lr: the learning rate\n",
        "            eps: the epsilon value\n",
        "        \"\"\"\n",
        "        # -----------------------------------#\n",
        "        # Finish defining q_value table.\n",
        "        # Hint: Q is a function of (s,a). The first four dimensions account\n",
        "        # for the discretized state, what should the last dimension be?\n",
        "        self.q_values = torch.zeros((30, 30, 50, 50, TODO))\n",
        "        # -----------------------------------#\n",
        "\n",
        "        self.lr = lr\n",
        "        self.discount = discount\n",
        "\n",
        "    def _get_q_idx(self, obs):\n",
        "      \"\"\"\n",
        "      Convert state continuous values to discrete Q-Table bins\n",
        "      Args:\n",
        "        obs   torch.Tensor(4,)  observation tensor\n",
        "      \"\"\"\n",
        "      # CartPole discretization constants\n",
        "      discrete_state = obs / torch.tensor([0.25, 0.25, 0.01, 0.1]) + torch.tensor([15, 10, 1, 10])\n",
        "      return tuple(discrete_state.int().tolist())\n",
        "\n",
        "    def _get_q(self, obs):\n",
        "      \"\"\"\n",
        "      Get Q-Table values\n",
        "      Args:\n",
        "        obs   torch.Tensor(4,)  observation tensor\n",
        "      Returns:\n",
        "              torch.Tensor(2, ) q-values for each action given obs\n",
        "      \"\"\"\n",
        "      idx = self._get_q_idx(obs)\n",
        "      return self.q_values[idx[0], idx[1], idx[2], idx[3]]\n",
        "\n",
        "    def _set_q(self, obs, action, value):\n",
        "      \"\"\"\n",
        "      Set Q-Table values\n",
        "      Args:\n",
        "        obs     torch.Tensor(4,)  observation tensor\n",
        "        action  int               action index\n",
        "        value   float             computed q-value\n",
        "      \"\"\"\n",
        "      idx = self._get_q_idx(obs)\n",
        "      self.q_values[idx[0], idx[1], idx[2], idx[3], action] = value\n",
        "\n",
        "\n",
        "    def action_step(self, obs: int) -> int:\n",
        "        \"\"\"\n",
        "        Greedy policy\n",
        "        Args:\n",
        "          obs   torch.Tensor(4,)  observation tensor\n",
        "        \"\"\"\n",
        "        if isinstance(obs, tuple): obs = torch.tensor(obs[0])\n",
        "        elif not isinstance(obs, torch.Tensor): obs = torch.tensor(obs)\n",
        "        # -----------------------------------#\n",
        "        # Greedily take the best action based on the q_values.\n",
        "        return int(TODO\n",
        "        # -----------------------------------#\n",
        "\n",
        "\n",
        "    def learner_step(\n",
        "        self,\n",
        "        obs: int,\n",
        "        action: int,\n",
        "        reward: float,\n",
        "        next_obs: int,\n",
        "    ):\n",
        "        \"\"\"Updates the Q-value of an action.\"\"\"\n",
        "        # -----------------------------------#\n",
        "        # temporal_difference = reward + self.discount *\n",
        "        # Hint: Use the definition given above for how to do the TD update.\n",
        "        temporal_difference = reward + self.discount * TODO\n",
        "        # -----------------------------------#\n",
        "        if not isinstance(obs, torch.Tensor):\n",
        "          obs = torch.tensor(obs)\n",
        "        if not isinstance(next_obs, torch.Tensor):\n",
        "          next_obs = torch.tensor(next_obs)\n",
        "\n",
        "        self._set_q(\n",
        "            obs=obs,\n",
        "            action=action,\n",
        "            value = self._get_q(obs)[action] + self.lr * temporal_difference\n",
        "        )"
      ],
      "metadata": {
        "id": "2EdKm3m6KzJY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 5e-1 # @param\n",
        "discount_factor = 0.97 # @param\n",
        "\n",
        "ql_policy = QLearningPolicy(\n",
        "    discount = discount_factor,\n",
        "    lr = learning_rate,\n",
        "    n_actions = env.action_space.n\n",
        ")"
      ],
      "metadata": {
        "id": "cDxh0oIMDDxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ⭐ Exercise\n",
        "\n",
        "Add the `learner_step` to the agent to complete the training loop."
      ],
      "metadata": {
        "id": "12xdoDEst0hL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "def train_policy(policy, num_episodes):\n",
        "  all_episode_rewards = []\n",
        "  progressbar = tqdm(range(num_episodes))\n",
        "  for i in progressbar: # iterate over the episodes\n",
        "    episode_rewards = []\n",
        "    done = False\n",
        "    obs = env.reset()\n",
        "    while not done: # iterate over the steps until termination\n",
        "        if isinstance(obs, tuple): obs = obs[0]\n",
        "        action = policy.action_step(obs) # consult q-function\n",
        "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
        "        # -----------------------------------#\n",
        "        policy.learner_step(TODO)\n",
        "        # -----------------------------------#\n",
        "        done = terminated\n",
        "        episode_rewards.append(reward) # compute discounted reward\n",
        "        obs = next_obs\n",
        "    all_episode_rewards.append(sum(episode_rewards))\n",
        "  return policy, all_episode_rewards"
      ],
      "metadata": {
        "id": "CdCIpmhhtysS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_steps = 1000 # @param\n",
        "ql_policy, ql_policy_rewards = train_policy(\n",
        "    policy=ql_policy, num_episodes=train_steps)"
      ],
      "metadata": {
        "id": "WUPIz5j9nab5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_rewards(ql_policy_rewards, 100)"
      ],
      "metadata": {
        "id": "rFUicqSqMZqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now have a trained Q-learning policy, that is a Q-table representing the action-state value function. Let's evaluate it."
      ],
      "metadata": {
        "id": "CqjrpF_GikW9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ql_eval_rewards, frames = evaluate_policy(policy=ql_policy, num_episodes=100)"
      ],
      "metadata": {
        "id": "XLh4bRqi9XKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_qlearning_policy_rwd = torch.tensor(ql_eval_rewards).mean()\n",
        "print(f\"Average evaluation reward\\n\")\n",
        "print(f\"Policy\\t\\t\\tAvg. reward\")\n",
        "print(f\"Random\\t\\t\\t{eval_random_policy_rwd}\")\n",
        "print(f\"Q-Learning Greedy\\t{eval_qlearning_policy_rwd}\")"
      ],
      "metadata": {
        "id": "I16Lg5tvjkYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "animate(frames)"
      ],
      "metadata": {
        "id": "c4-UVgqIPh1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How does the greedy agent perform wrt. the Random Policy?\n"
      ],
      "metadata": {
        "id": "ajuX5N4YkBrU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Epsilon-greedy agent\n",
        "\n",
        "So, most of the time, a Q-learning agent fails to solve the task. One of the reasons for it is that the greedy policy with respect to a given estimate of $Q^\\pi$ fails to *explore* the environment as needed. The problem is that our initialisation of the Q-function was \"pessimistic\" that means that once any of the actions is tried and is successful (by chance), the agent will keep selecting this action as it is \"greedily\" exploiting the Q-function estimate."
      ],
      "metadata": {
        "id": "4dgb7UNEMjjt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ⭐ Exercise\n",
        "\n",
        "One way to encourage exploration is to initialise the Q-funtion \"optimistically\", for example, with random values. Try modifying the agent above like this, what happens in this case? Hint: look at the line `self.q_values = torch.zeros((30, 30, 50, 50, env.action_space.n))`"
      ],
      "metadata": {
        "id": "SM8RBsjnXYyC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Modify in the `__init__` function:\n",
        "# self.q_values = torch.zeros((30, 30, 50, 50, env.action_space.n))"
      ],
      "metadata": {
        "id": "SVdMZXnrXbMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optimistic initialisation might help us in this case, but a more general solution is to use **epsilon-greedy agent**. An $\\epsilon$-greedy policy is a simple policy that at each time-step with probability $\\epsilon$ will choose a random action instead of the greedy action. This ensures constant exploration  during learning."
      ],
      "metadata": {
        "id": "U6pebAayXdbb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ⭐ Exercise\n",
        "\n",
        "Update the QlearningAgent's policy to an $\\epsilon$-greedy policy. Most things stays the same, but you need to modify `action_step` function to select the best action with probability $1-\\epsilon$ and a random action with probability $\\epsilon$. Note that agent takes parameters `epsilon` at the initialisation now."
      ],
      "metadata": {
        "id": "yGvt6yJabg1W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ECQPolicy:\n",
        "    def __init__(\n",
        "        self,\n",
        "        discount: float,\n",
        "        lr: float,\n",
        "        eps: float,\n",
        "        n_actions: int\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Implementation of the Q-Learning Policy\n",
        "        Args:\n",
        "            gamma: the discouting factor\n",
        "            lr: the learning rate\n",
        "            eps: the epsilon value\n",
        "        \"\"\"\n",
        "        # -----------------------------------#\n",
        "        # Use your implementation from the QLearningPolicy above.\n",
        "        size = (30, 30, 50, 50, TODO)\n",
        "        # Try different q_table initializations (zeros, ones, randn)\n",
        "        self.q_values = TODO\n",
        "        # -----------------------------------#\n",
        "\n",
        "        self.lr = lr\n",
        "        self.eps = eps\n",
        "        self.discount = discount\n",
        "\n",
        "    def _get_q_idx(self, obs):\n",
        "      \"\"\"\n",
        "      Convert state continuous values to discrete Q-Table bins\n",
        "      Args:\n",
        "        obs   torch.Tensor(4,)  observation tensor\n",
        "      \"\"\"\n",
        "      # specific to CartPole\n",
        "      discrete_state = obs / torch.tensor([0.25, 0.25, 0.01, 0.1]) + torch.tensor([15, 10, 1, 10])\n",
        "      return tuple(discrete_state.int().tolist())\n",
        "\n",
        "    def _get_q(self, obs):\n",
        "      \"\"\"\n",
        "      Get Q-Table values\n",
        "      Args:\n",
        "        obs   torch.Tensor(4,)  observation tensor\n",
        "      Returns:\n",
        "              torch.Tensor(2, ) q-values for each action given obs\n",
        "      \"\"\"\n",
        "      idx = self._get_q_idx(obs)\n",
        "      return self.q_values[idx[0], idx[1], idx[2], idx[3]]\n",
        "\n",
        "    def _set_q(self, obs, action, value):\n",
        "      \"\"\"\n",
        "      Set Q-Table values\n",
        "      Args:\n",
        "        obs     torch.Tensor(4,)  observation tensor\n",
        "        action  int               action index\n",
        "        value   float             computed q-value\n",
        "      \"\"\"\n",
        "      idx = self._get_q_idx(obs)\n",
        "      self.q_values[idx[0], idx[1], idx[2], idx[3], action] = value\n",
        "\n",
        "\n",
        "    def action_step(self, obs: int) -> int:\n",
        "        \"\"\"\n",
        "        Epsilon-greedy policy\n",
        "        Args:\n",
        "          obs   torch.Tensor(4,)  observation tensor\n",
        "        \"\"\"\n",
        "        if isinstance(obs, tuple): obs = torch.tensor(obs[0])\n",
        "        elif not isinstance(obs, torch.Tensor): obs = torch.tensor(obs)\n",
        "        # -----------------------------------#\n",
        "        if torch.rand(1).item() < self.eps:\n",
        "          # Act randomly with probability eps.\n",
        "          return TODO\n",
        "        else:\n",
        "          # Otherwise, what should we do?\n",
        "          return TODO\n",
        "        # -----------------------------------#\n",
        "\n",
        "\n",
        "    def learner_step(\n",
        "        self,\n",
        "        obs: int,\n",
        "        action: int,\n",
        "        reward: float,\n",
        "        next_obs: int,\n",
        "    ):\n",
        "\n",
        "        if not isinstance(obs, torch.Tensor):\n",
        "              obs = torch.tensor(obs)\n",
        "        if not isinstance(next_obs, torch.Tensor):\n",
        "          next_obs = torch.tensor(next_obs)\n",
        "\n",
        "        \"\"\"Updates the Q-value of an action.\"\"\"\n",
        "        temporal_difference = (\n",
        "            reward + self.discount * torch.max(self._get_q(next_obs)) - self._get_q(obs)[action]\n",
        "        )\n",
        "        self._set_q(\n",
        "            obs=obs,\n",
        "            action=action,\n",
        "            value = self._get_q(obs)[action] + self.lr * temporal_difference\n",
        "        )"
      ],
      "metadata": {
        "id": "7KbLdBuCXydM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.5 # @param\n",
        "discount_factor = 0.97 # @param\n",
        "eps = 0.3 # @param\n",
        "\n",
        "ql_policy = ECQPolicy(\n",
        "    eps = eps,\n",
        "    discount = discount_factor,\n",
        "    lr = learning_rate,\n",
        "    n_actions = env.action_space.n\n",
        ")"
      ],
      "metadata": {
        "id": "U3UMeEmVpzhj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "policy, train_rewards = train_policy(policy=ql_policy, num_episodes=1000)\n",
        "eval_rewards, frames = evaluate_policy(policy=policy, num_episodes=100)"
      ],
      "metadata": {
        "id": "_rbMvNAWqRGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_rewards(train_rewards, 100)"
      ],
      "metadata": {
        "id": "j-jA58lltMkI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_qlearning_policy_eps_decay_rwd = torch.tensor(eval_rewards).mean()\n",
        "print(f\"Average evaluation reward\\n\")\n",
        "print(f\"Policy\\t\\t\\tAvg. reward\")\n",
        "print(f\"Random\\t\\t\\t{eval_random_policy_rwd}\")\n",
        "print(f\"Q-Learning Greedy\\t{eval_qlearning_policy_rwd}\")\n",
        "print(f\"Q-Learning Eps-Greedy\\t{eval_qlearning_policy_eps_decay_rwd}\")"
      ],
      "metadata": {
        "id": "Dc8woIZsqcEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ⭐ Exercise\n",
        "Play with the different q-value intializations and record the average evaluation reward you get for the different policies!"
      ],
      "metadata": {
        "id": "4Bu0ZPA324Zi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| **Initialization** | **Avg. Reward for Random** | **Avg. Reward for Q-learning (greedy)** | **Avg. Reward for Q-learning ($\\epsilon$-greedy)** |\n",
        "| :--- | :--- | :--- | :--- |\n",
        "| **Zeros** | | | |\n",
        "| **Ones** | | | |\n",
        "| **Randn** | | | |"
      ],
      "metadata": {
        "id": "QBknefPZ2kaP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "animate(frames)"
      ],
      "metadata": {
        "id": "ePyRFWaAbGdq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ⭐ Exercise\n",
        "\n",
        "Does the agent manage to solve the task? Experiment with the number of training episodes, the exploration rate $\\epsilon$ and learning step.\n",
        "\n",
        "What configuration allows you to achieve the highest score?"
      ],
      "metadata": {
        "id": "BP5fJ5lsdXvx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ⭐ Bonus exercise\n",
        "\n",
        "Try the Epsilon-Greedy Agent on the `Minigolf` environment. Fill the blanks to adapt the `ECQPolicy`."
      ],
      "metadata": {
        "id": "DzuOu7H6dZzI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = Minigolf()"
      ],
      "metadata": {
        "id": "o_uxtCbVuJyf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MinigolfECQPolicy(ECQPolicy):\n",
        "    def __init__(\n",
        "        self,\n",
        "        discount: float,\n",
        "        lr: float,\n",
        "        eps: float,\n",
        "        n_bins: int\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Implementation of the Q-Learning Policy\n",
        "        Args:\n",
        "            gamma: the discouting factor\n",
        "            lr: the learning rate\n",
        "            eps: the epsilon value\n",
        "        \"\"\"\n",
        "        self.lr = lr\n",
        "        self.eps = eps\n",
        "        self.discount = discount\n",
        "\n",
        "        N_b = n_bins\n",
        "        self.dims = 3 # x, v, a\n",
        "        self.q_values = torch.zeros([N_b] * self.dims)\n",
        "        self.buckets = [\n",
        "            torch.linspace(env.min_pos, env.max_pos, N_b),\n",
        "            torch.linspace(env.min_friction, env.max_friction, N_b),\n",
        "            torch.linspace(env.min_action, env.max_action, N_b),\n",
        "        ]\n",
        "\n",
        "    def _get_q_idx(self, obs):\n",
        "      \"\"\"\n",
        "      Convert state continuous values to discrete Q-Table bins\n",
        "      Args:\n",
        "        obs   torch.Tensor(2,)  observation tensor\n",
        "        Returns:\n",
        "              List[int, int]    indices for discretized 2-dim observations\n",
        "      \"\"\"\n",
        "      # discretizing continuous observations with torch.bucketize!\n",
        "      return [torch.bucketize(obs[d], self.buckets[d]).item() for d in range(self.dims-1)]\n",
        "\n",
        "    def _get_q(self, obs):\n",
        "      \"\"\"\n",
        "      Get Q-Table values\n",
        "      Args:\n",
        "        obs   torch.Tensor(4,)  observation tensor\n",
        "      Returns:\n",
        "              torch.Tensor(2, ) q-values for each action given obs\n",
        "      \"\"\"\n",
        "      idx = self._get_q_idx(obs)\n",
        "      # -----------------------------------#\n",
        "      # the action space has changed! Check len(idx)\n",
        "      return TODO\n",
        "      # -----------------------------------#\n",
        "\n",
        "    def _set_q(self, obs, action, value):\n",
        "      \"\"\"\n",
        "      Set Q-Table values\n",
        "      Args:\n",
        "        obs     torch.Tensor(4,)  observation tensor\n",
        "        action  int               action index\n",
        "        value   float             computed q-value\n",
        "      \"\"\"\n",
        "      idx = self._get_q_idx(obs)\n",
        "      # -----------------------------------#\n",
        "      # this is similar, but one more index is required\n",
        "      TODO\n",
        "      # -----------------------------------#\n",
        "\n",
        "    def action_step(self, obs: int) -> int:\n",
        "        \"\"\"\n",
        "        Epsilon-greedy policy\n",
        "        Args:\n",
        "          obs   torch.Tensor(4,)  observation tensor\n",
        "        \"\"\"\n",
        "        if isinstance(obs, tuple): obs = torch.tensor(obs[0])\n",
        "        elif not isinstance(obs, torch.Tensor): obs = torch.tensor(obs)\n",
        "        # -----------------------------------#\n",
        "        if torch.rand(1).item() < self.eps:\n",
        "          return np.random.rand(*env.action_space.shape)\n",
        "        else:\n",
        "          return np.argmax(self._get_q(obs))\n",
        "        # -----------------------------------#"
      ],
      "metadata": {
        "id": "c_d1dWZNtTMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# play with this params!\n",
        "n_train_episodes = 1000 # @param\n",
        "n_eval_episodes = 100 # @param\n",
        "learning_rate = 0.5 # @param\n",
        "discount_factor = 0.97 # @param\n",
        "eps = 0.3 # @param\n",
        "n_bins = 50 # @param\n",
        "\n",
        "\n",
        "# -----------------------------------#\n",
        "# ql_policy = MinigolfECQPolicy(\n",
        "ql_policy = MinigolfECQPolicy(\n",
        "    TODO\n",
        ")\n",
        "# -----------------------------------#"
      ],
      "metadata": {
        "id": "pu6lDVl0s6P-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "policy, train_rewards = train_policy(policy=ql_policy, num_episodes=n_train_episodes)"
      ],
      "metadata": {
        "id": "yNemdc092e3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_rewards = evaluate_policy(policy=policy, num_episodes=n_eval_episodes)"
      ],
      "metadata": {
        "id": "KJ3hDqZp2fpz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_rewards(train_rewards, 100)"
      ],
      "metadata": {
        "id": "hWv-HQbF2hQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And that's it! You should observe an upward trend in the reward curve over time. Feel free to play with the params to e.g. see the impact of the discretization parameter `N_b`. You can now move to the next section."
      ],
      "metadata": {
        "id": "hqYzw2vz7r19"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🥳 Congratulations on completing the first part of this tutorial, great job!!!\n",
        "\n",
        "It is the most important part as it sets the foundations which would allow you to understand more advanced RL algorithms. Next, we will look into Deep RL: when RL algorithms are combined with neural network training."
      ],
      "metadata": {
        "id": "0cTs59rZdbyE"
      }
    }
  ]
}